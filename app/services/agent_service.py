import os
from typing import List, Dict, Annotated, TypedDict, Literal
import operator
import json
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langchain_aws import ChatBedrock

from langgraph.graph import StateGraph, START, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage

from app.core.config import settings
from app.services.tools.system_tools import get_skill_content
from app.services.tools.medical_tools import search_device_manual, get_user_health_data


# 1. å®šç¾© Graph ç‹€æ…‹
class AgentState(TypedDict):
    user_id: str
    input_message: str
    messages: Annotated[List[BaseMessage], operator.add]  # ç´¯åŠ å°è©±æ­·å²
    intent: Literal["device", "health", "general"]  # è·¯ç”±æ„åœ–
    is_emergency: bool  # æ–°å¢ï¼šç”¨æ–¼åˆ¤æ–·æ˜¯å¦è§¸ç™¼ç·Šæ€¥ç‹€æ…‹
    context_data: str  # å·¥å…·æŠ“å–çš„åŸå§‹æ•¸æ“š
    final_response: str  # æœ€çµ‚ç”¢å‡ºçš„å›è¦†


class AgentService:

    def __init__(self):
        # åˆå§‹åŒ– LLM (å‹•æ…‹é¸æ“‡ LLM)
        self.llm = self._get_llm()

        # è¼‰å…¥æŠ€èƒ½å…§å®¹
        self.skills = {
            "device": get_skill_content("device_expert"),
            "health": get_skill_content("health_analyst")
        }

        # æ§‹å»ºç”Ÿç”¢ç·š (Graph)
        self.workflow = self._build_workflow()
        self.app = self.workflow.compile()

        # ç°¡å–®è¨˜æ†¶é«”
        self.chat_history_map: Dict[str, List[BaseMessage]] = {}

    def _get_llm(self):
        """æ ¹æ“šç’°å¢ƒè®Šæ•¸è¿”å›å°æ‡‰çš„ LLM å¯¦ä¾‹"""
        # æ³¨æ„ï¼šé€šå¸¸ Embedding èˆ‡ LLM Provider æœƒè¨­ç‚ºåŒä¸€å€‹ï¼Œä½†ä¹Ÿå¯ä»¥åˆ†é–‹
        provider = os.getenv("LLM_PROVIDER", "google").lower()

        if provider == "google":
            return ChatGoogleGenerativeAI(
                model="gemini-2.0-flash",
                google_api_key=settings.gemini_api_key,
                temperature=0)
        elif provider == "openai":
            return ChatOpenAI(model="gpt-4o",
                              api_key=os.getenv("OPENAI_API_KEY"),
                              temperature=0)
        elif provider == "bedrock":
            # æœªä¾†ä¸Š AWS ä¹‹å¾Œçš„é…ç½®
            return ChatBedrock(
                model_id=
                "anthropic.claude-3-5-sonnet-20240620-v1:0",  # æˆ– Llama 3
                region_name=os.getenv("AWS_REGION", "us-east-1"),
                model_kwargs={"temperature": 0})
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ LLM Provider: {provider}")

    def _build_workflow(self):
        graph = StateGraph(AgentState)

        # å®šç¾©ç¯€é» (Nodes)
        graph.add_node("router", self.node_router)
        graph.add_node("device_expert", self.node_device_expert)
        graph.add_node("health_analyst", self.node_health_analyst)
        graph.add_node("emergency_advice", self.node_emergency_advice)
        graph.add_node("general_assistant", self.node_general_assistant)

        # å®šç¾©é‚Šèˆ‡æ¢ä»¶ (Edges & Conditional Edges)
        graph.add_edge(START, "router")

        # æ ¹æ“š router çš„æ„åœ–æ±ºå®šå»å‘
        graph.add_conditional_edges(
            "router", lambda state: state["intent"], {
                "device": "device_expert",
                "health": "health_analyst",
                "general": "general_assistant"
            })
        # å¥åº·åˆ†æå®Œå¾Œï¼Œåˆ¤æ–·æ˜¯å¦éœ€è¦ã€Œç·Šæ€¥å»ºè­°ã€
        graph.add_conditional_edges(
            "health_analyst", lambda state: "emergency"
            if state.get("is_emergency") else "normal", {
                "emergency": "emergency_advice",
                "normal": END
            })
        # å°ˆå®¶è™•ç†å®Œå¾Œå…¨éƒ¨æŒ‡å‘çµæŸ
        graph.add_edge("device_expert", END)
        graph.add_edge("emergency_advice", END)
        graph.add_edge("general_assistant", END)

        return graph

    # --- ç¯€é»é‚è¼¯å¯¦ä½œ ---

    def node_router(self, state: AgentState):
        """æ„åœ–è·¯ç”±ï¼šæ±ºå®šè©²æ‰¾å“ªä½å°ˆå®¶"""
        prompt = ("åˆ†æä»¥ä¸‹ç”¨æˆ¶è¨Šæ¯ï¼Œåˆ¤æ–·å…¶æ„åœ–ï¼š\n"
                  "1. å¦‚æœè©¢å•éŒ¯èª¤ä»£ç¢¼(ERR)ã€æ“ä½œã€èªªæ˜æ›¸ï¼Œå›å‚³ 'device'\n"
                  "2. å¦‚æœè©¢å•è¡€å£“æ•¸æ“šã€è¶¨å‹¢åˆ†æã€å¥åº·ç´€éŒ„ï¼Œå›å‚³ 'health'\n"
                  "3. å…¶ä»–é€šç”¨å¯’æš„ï¼Œå›å‚³ 'general'\n"
                  f"ç”¨æˆ¶è¨Šæ¯ï¼š{state['input_message']}\n"
                  "åƒ…å›å‚³å–®è©ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚")
        res = self.llm.invoke(prompt)
        intent = res.content.strip().lower()
        return {
            "intent": intent if intent in ["device", "health"] else "general"
        }

    async def node_device_expert(self, state: AgentState):
        """ç¡¬é«”å°ˆå®¶ç¯€é»ï¼šå°ˆæ³¨æ–¼ RAG æª¢ç´¢"""
        # åŸ·è¡Œå·¥å…· (é€™éƒ¨åˆ†å¯æ ¹æ“šä½ çš„ search_device_manual å¯¦ä½œèª¿æ•´)
        raw_info = await search_device_manual.ainvoke(
            {"query": state["input_message"]})

        prompt = (f"### å°ˆæ¥­åŸ·è¡Œç´°å‰‡ ###\n{self.skills['device']}\n\n"
                  f"### æª¢ç´¢åˆ°çš„èªªæ˜æ›¸è³‡è¨Š ###\n{raw_info}\n\n"
                  f"è«‹æ ¹æ“šä¸Šè¿°è¦ç¯„å›ç­”ç”¨æˆ¶ï¼š{state['input_message']}")
        res = await self.llm.ainvoke(prompt)
        return {"final_response": res.content}

    async def node_health_analyst(self, state: AgentState):
        """å¥åº·åˆ†æå¸«ç¯€é»ï¼šå°ˆæ³¨æ–¼æ•¸æ“šè™•ç†"""
        # èª¿ç”¨å·¥å…·ç²å–è¡€å£“æ•¸æ“š
        raw_data = get_user_health_data.invoke({"user_id": state["user_id"]})

        prompt = (f"### å°ˆæ¥­è¦ç¯„ ###\n{self.skills['health']}\n\n"
                  f"### çœŸå¯¦æ•¸æ“š ###\n{raw_data}\n\n"
                  f"### ç”¨æˆ¶ç•¶å‰æè¿° ###\n{state['input_message']}\n\n"
                  "1. è«‹çµåˆæ­·å²æ•¸æ“šèˆ‡ã€ç”¨æˆ¶ç•¶å‰æè¿°çš„æ•¸å€¼ã€é€²è¡Œç¶œåˆåˆ†æã€‚\n"
                  "2. è«‹æ ¹æ“šè¦ç¯„åˆ†ææ•¸æ“šã€‚è‹¥å‡ºç¾ä»»ä½•ä¸€é …ã€ç•°å¸¸ã€(BP, SpO2, Temp)ï¼Œ"
                  "è«‹åœ¨æ–‡æœ«æ¨™è¨» [EMERGENCY]ï¼Œå¦å‰‡æ¨™è¨» [NORMAL]ã€‚")
        res = await self.llm.ainvoke(prompt)

        data_list = json.loads(raw_data).get("history", [])
        can_visualize = len(data_list) >= 5

        is_emergency = "[EMERGENCY]" in res.content
        clean_content = res.content.replace("[EMERGENCY]",
                                            "").replace("[NORMAL]", "")
        if can_visualize:
            clean_content += "\n\nğŸ’¡ **ç³»çµ±åµæ¸¬åˆ°æ•¸æ“šé‡å……è¶³ï¼Œéœ€è¦æˆ‘ç‚ºæ‚¨ç¹ªè£½è¶¨å‹¢åˆ†æåœ–è¡¨å—ï¼Ÿ**"
        return {"final_response": clean_content, "is_emergency": is_emergency}

    async def node_emergency_advice(self, state: AgentState):
        """ç·Šæ€¥å»ºè­°ç¯€é»ï¼šè‡¨åºŠæŒ‡å¼•æ¨¡å¼"""
        prompt = ("### è‡¨åºŠé¢¨éšªè­¦ç¤º ###\n"
                  "ç•¶å‰æª¢æ¸¬åˆ°ç”¨æˆ¶è¡€å£“æ•¸æ“šå·²é”è‡¨åºŠè­¦æˆ’æ°´ä½ã€‚\n"
                  "è«‹æä¾›æ¨™æº–åŒ–çš„é†«å­¸å»ºè­°ï¼š\n"
                  "1. å»ºè­°ç”¨æˆ¶ä¿æŒå¹³éœï¼Œéœå 15 åˆ†é˜å¾Œé‡æ–°æ¸¬é‡ã€‚\n"
                  "2. è‹¥ä¼´éš¨é ­ç—›ã€èƒ¸ç—›ç­‰ç—‡ç‹€ï¼Œå»ºè­°ç«‹å³å°‹æ±‚å°ˆæ¥­é†«ç™‚å”åŠ©æˆ–æ’¥æ‰“ç·Šæ€¥é›»è©±ã€‚")
        res = await self.llm.ainvoke(prompt)
        combined = f"{state['final_response']}\n\n--- âš ï¸ ç³»çµ±è‡¨åºŠå»ºè­° ---\n{res.content}"

        return {"final_response": combined}

    async def node_general_assistant(self, state: AgentState):
        """é€šç”¨ç¯€é»ï¼šè™•ç†ç¯„ç–‡å¤–å•é¡Œ"""
        res = await self.llm.ainvoke(
            f"ä½ æ˜¯ä¸€ä½ç¦®è²Œçš„åŠ©æ‰‹ï¼Œè«‹å‘ŠçŸ¥ç”¨æˆ¶ä½ å°ˆæ³¨æ–¼å¥åº·æ•¸æ“šåˆ†ææˆ–è¨­å‚™èªªæ˜ï¼Œç„¡æ³•å›ç­”ä»¥ä¸‹å•é¡Œï¼š{state['input_message']}"
        )
        print(
            f"ğŸ§  [Router Decision] æ„åœ–è¾¨è­˜çµæœ: {state['intent']} (åŸå§‹è¨Šæ¯: {state['input_message']})"
        )
        return {"final_response": res.content}

    # --- API é€²å…¥é» ---

    async def handle_chat(self, user_id: str, message: str) -> str:
        # å–å¾—æ­·å²ç´€éŒ„
        history = self.chat_history_map.get(user_id, [])
        initial_state = {
            "user_id": user_id,
            "input_message": message,
            "messages": history + [HumanMessage(content=message)],
        }
        try:
            # å•Ÿå‹• LangGraph ç”Ÿç”¢ç·š
            final_state = await self.app.ainvoke(initial_state)
            final_text = final_state["final_response"]
            mermaid_graph = self.app.get_graph().draw_mermaid()
            if final_state.get("is_emergency"):
                mermaid_graph += "\nclass emergency_advice activeEmergencyNode"
            # æ ¼å¼åŒ–è¼¸å‡ºï¼ˆåŠ ä¸Šè¿½è¹¤è³‡è¨Šï¼Œæ–¹ä¾¿ç ”ç©¶åˆ†æï¼‰
            debug_info = f"\n\n--- ğŸ§  Agent è·¯ç”±è¿½è¹¤ ---\næ„åœ–ï¼š{final_state.get('intent')}\nç¯€é»è·¯å¾‘ï¼šRouter -> {final_state.get('intent')}_expert"
            # æ›´æ–°è¨˜æ†¶é«”
            self._update_history(user_id, message, final_text)
            # æ•´ç†å›å‚³çµæ§‹
            response_data = {
                "text": final_text,
                "graph": mermaid_graph,
                "intent": final_state.get("intent", "general")
            }
            return response_data

        except Exception as e:
            print(f"Graph Execution Error: {e}")
            return "åˆ†æéç¨‹å‡ºç¾ç•°å¸¸ï¼Œè«‹æª¢æŸ¥è¨­å‚™é€£ç·šã€‚"

    def _update_history(self, user_id: str, user_msg: str, ai_msg: str):
        history = self.chat_history_map.get(user_id, [])
        history.append(HumanMessage(content=user_msg))
        history.append(AIMessage(content=ai_msg))
        self.chat_history_map[user_id] = history[-10:]


def save_graph_image(agent_service):
    try:
        # å–å¾—ç·¨è­¯å¾Œçš„åœ–çµæ§‹ï¼Œä¸¦è½‰æ›ç‚º Mermaid æ ¼å¼çš„ PNG
        graph_image = agent_service.app.get_graph().draw_mermaid_png()

        with open("agent_workflow.png", "wb") as f:
            f.write(graph_image)
        print("âœ… æµç¨‹åœ–å·²æˆåŠŸå„²å­˜ç‚º agent_workflow.png")
    except Exception as e:
        print(f"ç„¡æ³•ç”¢ç”Ÿåœ–ç‰‡ï¼Œè«‹ç¢ºä¿å®‰è£äº†å¿…è¦å¥—ä»¶: {e}")
