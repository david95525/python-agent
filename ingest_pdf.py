import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_postgres.vectorstores import PGVector
from sqlalchemy import create_engine, text

# æ ¹æ“šéœ€æ±‚å‹•æ…‹è¼‰å…¥ä¸åŒçš„ Embedding
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain_aws import BedrockEmbeddings  # æœªä¾†å°æ¥ AWS çš„é—œéµ

from app.core.config import settings

load_dotenv()


def get_embeddings():
    """é€šç”¨ Embedding é¸æ“‡å™¨"""
    provider = os.getenv("EMBEDDING_PROVIDER", "google").lower()

    if provider == "google":
        return GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    elif provider == "openai":
        return OpenAIEmbeddings(model="text-embedding-3-small")
    elif provider == "bedrock":
        # æœªä¾†é·ç§»åˆ° AWS æ™‚åªéœ€æ”¹ç’°å¢ƒè®Šæ•¸
        return BedrockEmbeddings(region_name=os.getenv("AWS_REGION",
                                                       "us-east-1"),
                                 model_id="amazon.titan-embed-text-v2:0")
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„ Provider: {provider}")


def run_ingest():
    pdf_path = "data/bp.pdf"
    provider = os.getenv("EMBEDDING_PROVIDER", "google")
    collection_name = f"microlife_docs_{provider}"

    try:
        # 1. å»ºç«‹åŒæ­¥é€£ç·šæ¸…ç†èˆŠè³‡æ–™
        sync_url = settings.database_url.replace("postgresql+psycopg",
                                                 "postgresql")
        engine = create_engine(sync_url)

        print(f"ğŸ§¹ æ¸…ç† {collection_name} ä¸­çš„èˆŠè³‡æ–™...")
        with engine.connect() as conn:
            # ä½¿ç”¨æ›´å®‰å…¨çš„èªæ³•æ¸…ç†ç‰¹å®š collection
            conn.execute(
                text("""
                DELETE FROM langchain_pg_embedding 
                WHERE collection_id = (SELECT uuid FROM langchain_pg_collection WHERE name = :name)
            """), {"name": collection_name})
            conn.commit()

        # 2. è¼‰å…¥èˆ‡å¼·åŒ–åˆ‡ç‰‡
        loader = PyPDFLoader(pdf_path)
        raw_docs = loader.load()

        # é‡å°èªªæ˜æ›¸å„ªåŒ–ï¼šä¿æŒæ®µè½å®Œæ•´æ€§
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=120,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""])
        docs = splitter.split_documents(raw_docs)

        # 3. ç²å–é€šç”¨ Embedding ä¸¦å­˜å…¥
        embeddings = get_embeddings()
        print(f"ğŸ§  ä½¿ç”¨ {provider} ç”Ÿæˆå‘é‡ä¸­...")

        PGVector.from_documents(
            embedding=embeddings,
            documents=docs,
            collection_name=collection_name,
            connection=settings.database_url,
            use_jsonb=True,
        )
        print(f"âœ… æˆåŠŸï¼è³‡æ–™å·²å­˜å…¥ Collection: {collection_name}")

    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")


if __name__ == "__main__":
    run_ingest()
